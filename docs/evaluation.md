# Evaluation

How we measure what language models can and cannot do. This section includes major benchmarks, evaluation frameworks and methodology, LLM-as-a-judge approaches, and work on uncertainty and calibration. Rigorous evaluation is critical for understanding progress and identifying failure modes.

## Benchmarks

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2009.03300">Measuring Massive Multitask Language Understanding</a></td>
    <td>Hendrycks et al.</td>
    <td>2020</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2110.14168">Training Verifiers to Solve Math Word Problems</a></td>
    <td>Cobbe et al.</td>
    <td>2021</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2310.06770">SWE-bench: Can Language Models Resolve Real-World GitHub Issues?</a></td>
    <td>Jimenez et al.</td>
    <td>2023</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2107.03374">Evaluating Large Language Models Trained on Code</a></td>
    <td>Chen et al.</td>
    <td>2021</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2311.12983">GAIA: A Benchmark for General AI Assistants</a></td>
    <td>Mialon et al.</td>
    <td>2023</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2311.12022">GPQA: A Graduate-Level Google-Proof Q&A Benchmark</a></td>
    <td>Rein et al.</td>
    <td>2023</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2109.07958">TruthfulQA: Measuring How Models Mimic Human Falsehoods</a></td>
    <td>Lin et al.</td>
    <td>2021</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/1803.05457">Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge</a></td>
    <td>Clark et al.</td>
    <td>2018</td>
  </tr>
</table>

## Evaluation methodology

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2211.09110">Holistic Evaluation of Language Models</a></td>
    <td>Liang et al.</td>
    <td>2022</td>
  </tr>
</table>

## LLM-as-a-judge

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2306.05685">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a></td>
    <td>Zheng et al.</td>
    <td>2023</td>
  </tr>
</table>

## Uncertainty and calibration

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2205.14334">Teaching Models to Express Their Uncertainty in Words</a></td>
    <td>Lin et al.</td>
    <td>2022</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2305.14975">Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</a></td>
    <td>Turpin et al.</td>
    <td>2023</td>
  </tr>
</table>
