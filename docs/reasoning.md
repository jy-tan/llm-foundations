# Reasoning

Techniques for improving language model reasoning, primarily through additional inference-time compute. Covers prompting strategies (chain-of-thought, decomposition), in-context learning, self-consistency, and augmentation via retrieval and tools. Also includes work on world models - what models learn about how the world works.

## Chain-of-thought

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></td>
    <td>Wei et al.</td>
    <td>2022</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2205.11916">Large Language Models are Zero-Shot Reasoners</a></td>
    <td>Kojima et al.</td>
    <td>2022</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2305.10601">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</a></td>
    <td>Yao et al.</td>
    <td>2023</td>
  </tr>
</table>

## In-context learning

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a></td>
    <td>Brown et al.</td>
    <td>2020</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2202.12837">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</a></td>
    <td>Min et al.</td>
    <td>2022</td>
  </tr>
</table>

## Self-consistency & verification

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2203.11171">Self-Consistency Improves Chain of Thought Reasoning in Language Models</a></td>
    <td>Wang et al.</td>
    <td>2022</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2305.20050">Let's Verify Step by Step</a></td>
    <td>Lightman et al.</td>
    <td>2023</td>
  </tr>
</table>

## Task decomposition

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2205.10625">Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</a></td>
    <td>Zhou et al.</td>
    <td>2022</td>
  </tr>
</table>

## Augmentation & grounding

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2302.04761">Toolformer: Language Models Can Teach Themselves to Use Tools</a></td>
    <td>Schick et al.</td>
    <td>2023</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a></td>
    <td>Yao et al.</td>
    <td>2022</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2005.11401">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></td>
    <td>Lewis et al.</td>
    <td>2020</td>
  </tr>
</table>

## World models

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2310.02207">Language Models Represent Space and Time</a></td>
    <td>Gurnee et al.</td>
    <td>2023</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2310.06692">From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought</a></td>
    <td>Wong et al.</td>
    <td>2023</td>
  </tr>
</table>
