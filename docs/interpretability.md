# Interpretability

Understanding what happens inside neural networks. This includes mechanistic interpretability (circuits, features, superposition), and model editing techniques. Interpretability research aims to make models less opaque, enabling debugging, safety analysis, and scientific understanding of learned representations.

## Mechanistic interpretability

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework for Transformer Circuits</a></td>
    <td>Elhage et al.</td>
    <td>2021</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2209.11895">In-context Learning and Induction Heads</a></td>
    <td>Olsson et al.</td>
    <td>2022</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2209.10652">Toy Models of Superposition</a></td>
    <td>Elhage et al.</td>
    <td>2022</td>
  </tr>
  <tr>
    <td><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a></td>
    <td>Templeton et al.</td>
    <td>2024</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2309.00941">Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</a></td>
    <td>Bricken et al.</td>
    <td>2023</td>
  </tr>
</table>

## Model editing

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2202.05262">Locating and Editing Factual Associations in GPT</a></td>
    <td>Meng et al.</td>
    <td>2022</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2210.07229">Mass-Editing Memory in a Transformer</a></td>
    <td>Meng et al.</td>
    <td>2022</td>
  </tr>
</table>
