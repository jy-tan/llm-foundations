# Data

Training data is the foundation of model quality. This section covers tokenization algorithms, data curation and filtering, mixing strategies across domains, and deduplication techniques. Data decisions frequently matter more than architectural tweaks for final model performance.

## Tokenization

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/1508.07909">Neural Machine Translation of Rare Words with Subword Units</a></td>
    <td>Sennrich et al.</td>
    <td>2015</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/1808.06226">SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</a></td>
    <td>Kudo et al.</td>
    <td>2018</td>
  </tr>
</table>

## Data curation & quality

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2101.00027">The Pile: An 800GB Dataset of Diverse Text for Language Modeling</a></td>
    <td>Gao et al.</td>
    <td>2020</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2402.00159">Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research</a></td>
    <td>Soldaini et al.</td>
    <td>2024</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2406.17557">FineWeb: decanting the web for the finest text data at scale</a></td>
    <td>Penedo et al.</td>
    <td>2024</td>
  </tr>
</table>

## Data mixing

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2305.10429">DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining</a></td>
    <td>Xie et al.</td>
    <td>2023</td>
  </tr>
</table>

## Deduplication

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2107.06499">Deduplicating Training Data Makes Language Models Better</a></td>
    <td>Lee et al.</td>
    <td>2021</td>
  </tr>
</table>
