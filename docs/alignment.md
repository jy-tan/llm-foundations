# Alignment

Techniques for making models helpful, harmless, and honest. This covers reinforcement learning from human feedback (RLHF), direct preference optimization, constitutional and self-alignment approaches, and red-teaming for safety. These methods bridge the gap between raw capabilities and actually useful, safe behavior.

## RLHF and preference optimization

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/1706.03741">Deep Reinforcement Learning from Human Preferences</a></td>
    <td>Christiano et al.</td>
    <td>2017</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2009.01325">Learning to Summarize from Human Feedback</a></td>
    <td>Stiennon et al.</td>
    <td>2020</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2203.02155">Training Language Models to Follow Instructions with Human Feedback</a></td>
    <td>Ouyang et al.</td>
    <td>2022</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></td>
    <td>Rafailov et al.</td>
    <td>2023</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2402.01306">KTO: Model Alignment as Prospect Theoretic Optimization</a></td>
    <td>Ethayarajh et al.</td>
    <td>2024</td>
  </tr>
</table>

## Constitutional AI / self-alignment

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2112.00861">A General Language Assistant as a Laboratory for Alignment</a></td>
    <td>Askell et al.</td>
    <td>2021</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2212.08073">Constitutional AI: Harmlessness from AI Feedback</a></td>
    <td>Bai et al.</td>
    <td>2022</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2401.01335">Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models</a></td>
    <td>Chen et al.</td>
    <td>2024</td>
  </tr>
</table>

## Red-teaming

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2209.07858">Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned</a></td>
    <td>Perez et al.</td>
    <td>2022</td>
  </tr>
</table>
