# Fine-tuning / Adaptation

Techniques for adapting pretrained models to specific tasks or behaviors. This includes parameter-efficient methods (adapters, LoRA) that reduce compute costs, instruction tuning approaches that teach models to follow directions, and strategies for continual learning without catastrophic forgetting.

## Parameter-efficient fine-tuning

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/1902.00751">Parameter-Efficient Transfer Learning for NLP</a></td>
    <td>Houlsby et al.</td>
    <td>2019</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a></td>
    <td>Hu et al.</td>
    <td>2021</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a></td>
    <td>Dettmers et al.</td>
    <td>2023</td>
  </tr>
</table>

## Instruction tuning

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2109.01652">Finetuned Language Models Are Zero-Shot Learners</a></td>
    <td>Wei et al.</td>
    <td>2021</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a></td>
    <td>Ouyang et al.</td>
    <td>2022</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2212.10560">Self-Instruct: Aligning Language Models with Self-Generated Instructions</a></td>
    <td>Wang et al.</td>
    <td>2023</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2310.16944">Zephyr: Direct Distillation of LM Alignment</a></td>
    <td>Tunstall et al.</td>
    <td>2023</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2305.11206">LIMA: Less Is More for Alignment</a></td>
    <td>Zhou et al.</td>
    <td>2023</td>
  </tr>
</table>

## Continual learning

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/1612.00796">Overcoming catastrophic forgetting in neural networks</a></td>
    <td>Kirkpatrick et al.</td>
    <td>2017</td>
  </tr>
</table>
