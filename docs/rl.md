# Reinforcement Learning

Reinforcement learning as a capability amplifier for language models. This includes foundational RL algorithms relevant to LLMs, RL techniques for improving reasoning (process reward models, GRPO), and research on reward modeling and its pitfalls. RL increasingly drives the frontier of model capabilities.

## Foundations

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/1712.01815">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</a></td>
    <td>Silver et al.</td>
    <td>2017</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/1911.08265">Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</a></td>
    <td>Schrittwieser et al.</td>
    <td>2020</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2106.01345">Decision Transformer: Reinforcement Learning via Sequence Modeling</a></td>
    <td>Chen et al.</td>
    <td>2021</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2111.00210">Mastering Atari Games with Limited Data</a></td>
    <td>Ye et al.</td>
    <td>2021</td>
  </tr>
</table>

## RL for reasoning

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2305.20050">Let's Verify Step by Step</a></td>
    <td>Lightman et al.</td>
    <td>2023</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2312.08935">Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations</a></td>
    <td>Wang et al.</td>
    <td>2023</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2501.12948">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></td>
    <td>DeepSeek-AI</td>
    <td>2025</td>
  </tr>
</table>

## Reward modeling

<table>
  <tr>
    <th>Title</th>
    <th>Authors</th>
    <th>Year</th>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2210.10760">Scaling Laws for Reward Model Overoptimization</a></td>
    <td>Gao et al.</td>
    <td>2022</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2310.12921">Reward Model Ensembles Help Mitigate Overoptimization</a></td>
    <td>Coste et al.</td>
    <td>2023</td>
  </tr>
</table>
